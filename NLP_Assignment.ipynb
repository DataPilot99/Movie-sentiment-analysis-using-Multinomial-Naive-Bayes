{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmbsLd4KKX1f"
      },
      "source": [
        "# 1. Data Acquisition"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Import libraries"
      ],
      "metadata": {
        "id": "rXDx2qmk3X5X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtQkWKkf26Ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8c74c557-0d7f-4e08-cb61-1e959e9b209f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.1\n",
            "    Uninstalling scipy-1.16.1:\n",
            "      Successfully uninstalled scipy-1.16.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "scipy"
                ]
              },
              "id": "b9ad1278486d40508a88b8ef747866da"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4116577338.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install gensim'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# ML models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/corpora/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# bring corpus classes directly into package namespace, to save some typing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mindexedcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexedCorpus\u001b[0m  \u001b[0;31m# noqa:F401 must appear before the other classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmmcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMmCorpus\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/corpora/indexedcorpus.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/interfaces.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/matutils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m     \u001b[0;31m# try to load fast, cythonized code if possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_absolute_difference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirichlet_expectation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/_matutils.pyx\u001b[0m in \u001b[0;36minit gensim._matutils\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ],
      "source": [
        "import tarfile\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Pre-processing libraries\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Import Vectorizers\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "!pip install gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# ML models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# For evaluation of models\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqoHMzE13Ao2"
      },
      "outputs": [],
      "source": [
        "folder_path = '/content/drive/MyDrive/Atomcamp/aclImdb_v1.tar.gz'\n",
        "\n",
        "with tarfile.open(folder_path, 'r:gz') as tar:\n",
        "  tar.extractall(path='/content/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKWebnlKLKdq"
      },
      "source": [
        "### 1.2 Extract and Load the dataset\n",
        "The test folder has sub folders pos and neg, and both of them contain txt files, one txt file for each review. I will make a function to load all the txt files to a pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVCdCiBZ2wWk"
      },
      "outputs": [],
      "source": [
        "def load_data(folder_path, label):\n",
        "  data = []\n",
        "  for file in os.listdir(folder_path):\n",
        "    file_path = os.path.join(folder_path, file)\n",
        "    with open(file_path, 'r') as file_content:\n",
        "      review = file_content.read()\n",
        "      data.append((review, label))\n",
        "  return data\n",
        "\n",
        "# Train data\n",
        "neg_data = load_data('/content/aclImdb/train/neg', 0)\n",
        "pos_data = load_data('/content/aclImdb/train/pos', 1)\n",
        "train_data = neg_data + pos_data\n",
        "\n",
        "# Test data\n",
        "neg_data = load_data('/content/aclImdb/test/neg', 0)\n",
        "pos_data = load_data('/content/aclImdb/test/pos', 1)\n",
        "test_data = neg_data + pos_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybZ0CGlGCZzT"
      },
      "source": [
        "### 1.3 Clean HTML Tags and create DataFrame of the dataset\n",
        "When I explored the data, there were html tags present in the reviews. I will used re library to match the html tags and remove them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyjoCQ3wZV4l"
      },
      "outputs": [],
      "source": [
        "# Making a DataFrame for train data\n",
        "df_train = pd.DataFrame(train_data, columns=['Review', 'Label'])\n",
        "df_train['Review'] = df_train['Review'].apply(lambda x: re.sub(r'<.*?>', '', x))\n",
        "\n",
        "# Making a DataFrame for test data\n",
        "df_test = pd.DataFrame(test_data, columns=['Review', 'Label'])\n",
        "df_test['Review'] = df_test['Review'].apply(lambda x: re.sub(r'<.*?>', '', x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5A0l4o6kZbZE"
      },
      "outputs": [],
      "source": [
        "df_test.head(2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtEfkgG5CrGN"
      },
      "outputs": [],
      "source": [
        "df_train.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVJZoMaE8w-W"
      },
      "source": [
        "# 2. Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVW3-iaKbyld"
      },
      "outputs": [],
      "source": [
        "print('Length of train data: ', len(df_train))\n",
        "print('Positive Classes: ', len(df_train[df_train['Label']==1]))\n",
        "print('Negative Classes: ', len(df_train[df_train['Label']==0]))\n",
        "\n",
        "print('\\nLength of test data: ', len(df_test))\n",
        "print('Positive Classes: ', len(df_test[df_test['Label']==1]))\n",
        "print('Negative Classes: ', len(df_test[df_test['Label']==0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data splitting\n",
        "\n",
        "The orignal data has equal samples in test and train data (25000 samples each). However, since it is required to split the data into train, validataion and test in 70:10:20 ratio, I'll first combine all the data, and then split according to this ratio."
      ],
      "metadata": {
        "id": "LP-fqPn-2vYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_combined = pd.concat([df_train, df_test], ignore_index=True)\n",
        "df_train, df_val_test = train_test_split(df_combined, random_state=42, test_size=0.3, stratify=df_combined['Label'])\n",
        "df_val, df_test = train_test_split(df_val_test, random_state=42, stratify=df_val_test['Label'], test_size=2/3)\n",
        "\n",
        "print('Test data length: ', len(df_train))\n",
        "print('Validation data length: ', len(df_val))\n",
        "print('Test data length: ', len(df_test))\n",
        "\n"
      ],
      "metadata": {
        "id": "vwYcqCNG3QV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0T-wrGGDcMt"
      },
      "source": [
        "# 3. Preprocessing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lttGXsTTAV1T"
      },
      "outputs": [],
      "source": [
        "# Pre-processing function\n",
        "def preprocess_text(text):\n",
        "  # Convert to lowercase\n",
        "  text = text.lower()\n",
        "  # Remove special characters and digits\n",
        "  text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "  # Tokenize\n",
        "  tokens = word_tokenize(text)\n",
        "  # Stopwords removal\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [word for word in tokens if word not in stop_words]\n",
        "  # Stemming the tokens\n",
        "  stemmer = PorterStemmer()\n",
        "  stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "  return ' '.join(stemmed_tokens)\n",
        "  # Lemmetization of tokens\n",
        "  # lemmatizer = WordNetLemmatizer()\n",
        "  # lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]\n",
        "\n",
        "  #return stemmed_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHGz-KKeWY1w"
      },
      "outputs": [],
      "source": [
        "df_train['Review'] = df_train['Review'].apply(preprocess_text)\n",
        "\n",
        "df_val['Review'] = df_val['Review'].apply(preprocess_text)\n",
        "\n",
        "df_test['Review'] = df_test['Review'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bAVwj7-W6k0"
      },
      "outputs": [],
      "source": [
        "df_train.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5G2CijPU4obR"
      },
      "source": [
        "The dataset is now clean. I will now proceed to feature enengineering and modeling part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRGrLVtu4OFL"
      },
      "source": [
        "# 3. Feature Engineering\n",
        "\n",
        "### 3.1 Extracting features with BoW and TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oxGjHo_gFfE"
      },
      "source": [
        "I will extract features using Bag of Words and TF-IDF Vectorizers, considering both unigrams and bigrams, so that more meaningful features are extracted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyvzUwhM9t2C"
      },
      "outputs": [],
      "source": [
        "# Representation in CountVectorizer\n",
        "count_vec = CountVectorizer(ngram_range=(1,2))\n",
        "\n",
        "X_train_count = count_vec.fit_transform(df_train['Review'])\n",
        "X_val_count = count_vec.transform(df_val['Review'])\n",
        "X_test_count = count_vec.transform(df_test['Review'])\n",
        "\n",
        "# Representation in TF-IDF Vectorizer\n",
        "tfidf_vec = TfidfVectorizer(ngram_range=(1,2))\n",
        "\n",
        "X_train_tfidf = tfidf_vec.fit_transform(df_train['Review'])\n",
        "X_val_tfidf = tfidf_vec.transform(df_val['Review'])\n",
        "X_test_tfidf = tfidf_vec.transform(df_test['Review'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4UcxMTBQkEb"
      },
      "outputs": [],
      "source": [
        "print(X_train_count.shape)\n",
        "print(X_train_tfidf.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-I9V7kkkDQU"
      },
      "source": [
        "By `print(X_train_count.shape)`, we got to know that 1722012 features were extracted from 25000 documents.\n",
        "\n",
        "These features represent the vocabulary size (unigrams and/or n-grams) for vectorization.\n",
        "\n",
        "Now, I will use a pretrained embedding to extract features using Word2Vec\n",
        "\n",
        "### 3.2 Extracting features with Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_cXDoP0dZY2"
      },
      "source": [
        "First, I will train Word2Vec model on the test set vocabulary (Custom training), which means `w2v` will contain all the trained model parameters, the vocabulary it learned from the test data, and the word vectors of the vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsrtPqt5S233"
      },
      "outputs": [],
      "source": [
        "# Preprocessing train data for Word2Vec\n",
        "preprocessed_train_set = [review.split() for review in df_train['Review']]\n",
        "\n",
        "preprocessed_test_set = [review.split() for review in df_test['Review']]\n",
        "\n",
        "# Training Word2Vec model\n",
        "w2v = Word2Vec(sentences=preprocessed_train_set, vector_size=5, window=2, min_count=1, workers=0)\n",
        "\n",
        "'''# Displaying vector for a word\n",
        "print(w2v.wv['natural'])'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8ON6GqKxp55"
      },
      "outputs": [],
      "source": [
        "# Making a function for averaging vectors in a single doc to obtain document-level vector embeddings\n",
        "def average_vectors_in_doc(doc, model, vector_size):\n",
        "  vectors = [model.wv[word] for word in doc if word in model.wv]  # Fetches vector of every word in the doc\n",
        "  if vectors:\n",
        "    return np.mean(vectors, axis=0)\n",
        "  else:\n",
        "    return np.zeros(vector_size) # If all token removed from review during preprocessing, this line implements\n",
        "\n",
        "# Now passing every review to this function\n",
        "X_train_dense = np.array([average_vectors_in_doc(review, w2v, 5) for review in preprocessed_train_set])\n",
        "\n",
        "X_test_dense = np.array([average_vectors_in_doc(review, w2v, 5) for review in preprocessed_test_set])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A61q28lZ2CGj"
      },
      "outputs": [],
      "source": [
        "print(X_train_dense.shape)\n",
        "print(X_test_dense.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Multinomial Naive Bayes model\n"
      ],
      "metadata": {
        "id": "1L_V_tik-pWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting the target labels from the dataframes\n",
        "y_train = df_train['Label']\n",
        "y_val = df_val['Label']\n",
        "y_test = df_test['Label']"
      ],
      "metadata": {
        "id": "dDYBUdij-pA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Train Naive Bayes on CountVectorizer"
      ],
      "metadata": {
        "id": "S4P3444o-7AJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the model\n",
        "nb_classifier_count = MultinomialNB()\n",
        "nb_classifier_count.fit(X_train_count, y_train)\n",
        "\n",
        "# Predicting on the test set\n",
        "y_pred = nb_classifier_count.predict(X_test_count)\n",
        "accuracy_nb_count = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(accuracy_nb_count)\n",
        "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))\n"
      ],
      "metadata": {
        "id": "_qYBTRp3-94h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Train Naive Bayes on IF-IDF"
      ],
      "metadata": {
        "id": "prsoLIGF--o3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the model\n",
        "nb_classifier_tfidf = MultinomialNB()\n",
        "nb_classifier_tfidf.fit(X_train_count, y_train)\n",
        "\n",
        "# Predicting on the test set\n",
        "y_pred = nb_classifier_tfidf.predict(X_test_count)\n",
        "accuracy_nb_tfidf = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(accuracy_nb_tfidf)\n",
        "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))\n"
      ],
      "metadata": {
        "id": "Z2gpIXMJcC8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. LinearSVM Model\n",
        "\n",
        "### 5.1 Train LinearSVM on CountVectorizer"
      ],
      "metadata": {
        "id": "1S_6HjnX_GMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm_count = LinearSVC()\n",
        "svm_count.fit(X_train_count, y_train)\n",
        "\n",
        "y_pred = svm_count.predict(X_test_count)\n",
        "accuracy_count_svm = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Linear SVM with CountVectorizer:\")\n",
        "print(\"Accuracy:\", accuracy_count_svm)\n",
        "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))\n"
      ],
      "metadata": {
        "id": "bpCnhB9l_I-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Train LinearSVM on TF-IDF"
      ],
      "metadata": {
        "id": "X33e7u2XYJCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm_tfidf = LinearSVC()\n",
        "svm_tfidf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_pred = svm_tfidf.predict(X_test_tfidf)\n",
        "accuracy_tfidf_svm = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Linear SVM with TF-IDF:\")\n",
        "print(\"Accuracy:\", accuracy_tfidf_svm)\n",
        "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))"
      ],
      "metadata": {
        "id": "eZzEQ1vTYNVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 Train LinearSVM with Word2Vec"
      ],
      "metadata": {
        "id": "O5M68YE_ZSYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm_w2v = LinearSVC()\n",
        "svm_w2v.fit(X_train_dense, y_train)\n",
        "\n",
        "y_pred_w2v_svm = svm_w2v.predict(X_test_dense)\n",
        "accuracy_w2v_svm = accuracy_score(y_test, y_pred_w2v_svm)\n",
        "\n",
        "print(\"Linear SVM with Word2Vec:\")\n",
        "print(\"Accuracy:\", accuracy_w2v_svm)\n",
        "print(classification_report(y_test, y_pred_w2v_svm, target_names=['Negative', 'Positive']))\n"
      ],
      "metadata": {
        "id": "iA9CAMb7ZWE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Models comparision"
      ],
      "metadata": {
        "id": "nJFovqIfbaqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = {\n",
        "    \"Feature Representation\": [\"Bag of Words\", \"TF-IDF\", \"Bag of Words\", \"TF-IDF\", \"Word Embeddings\"],\n",
        "    \"Model\": [\"Naive Bayes\", \"Naive Bayes\", \"LinearSVM\", \"LinearSVM\", \"LinearSVM\"],\n",
        "    \"Accuracy Score\": [accuracy_nb_count, accuracy_nb_tfidf, accuracy_count_svm, accuracy_tfidf_svm, accuracy_w2v_svm]\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "nGOcBh3Cam2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Analysis and Discussion\n",
        "**7.1 Compare generative vs. discriminative performance.**\n",
        "\n",
        "Overall, LinearSVM (the discriminative model) has performed better than Naive Bayes (the generative model). It's accuracy is higher in the sparse representations. However, it has performed very low in the dense vector representation.\n",
        "\n",
        "**7.2 Discuss how N‑gram size and embedding choice affected results.**"
      ],
      "metadata": {
        "id": "2rc9RMQdbeSb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qOfSFClsbgjb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}